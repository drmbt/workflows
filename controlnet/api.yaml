name: Controlnet
descriptin: Generate a new image which adheres to the shape or outline of an input image
tip: This tool is best for text-to-image when an input image is provided and the user wants to maintain the content of the input image while generating a new style or aesthetic on top of it.
output_type: image
handler: comfyui
comfyui_output_node_id: 2
parameters:
- name: prompt
  label: Prompt
  description: Text prompt
  type: string
  required: true
  comfyui: 
    node_id: 1
    field: inputs
    subfield: positive
- name: negative
  label: Negative prompt
  description: Negative text prompt, what you do *not* want to generate. You usually want to keep this at default or at most, add stuff to this prompt.
  type: string
  comfyui: 
    node_id: 1
    field: inputs
    subfield: negative
- name: width
  label: Width
  description: Width in pixels
  type: int
  default: 1024
  minimum: 256
  maximum: 2048
  step: 8
  comfyui: 
    node_id: 1
    field: inputs
    subfield: width
- name: height
  label: Height
  description: Height in pixels
  type: int
  default: 1024
  minimum: 256
  maximum: 2048
  step: 8
  comfyui: 
    node_id: 1
    field: inputs
    subfield: height
- name: seed
  label: Seed
  description: Set random seed for reproducibility. If blank, will be set to -1 to supply a random value.
  tip: You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this blank! 
  type: int
  default: random
  minimum: 0
  maximum: 10000000000000000
  comfyui: 
    node_id: 1
    field: inputs
    subfield: seed
- name: init_image
  label: Starting image
  description: Initial image from which to start diffusion process
  type: image
  required: true
  comfyui: 
    node_id: 28
    field: inputs
    subfield: image
- name: denoise
  label: Denoise strength
  description: Strength of denoising
  tip: Decreasing denoise strength increases the influence of the init image
  type: float
  default: 1.0
  minimum: 0.0
  maximum: 1.0
  comfyui: 
    node_id: 11
    field: inputs
    subfield: denoise
- name: steps
  label: Steps
  description: Length of diffusion steps
  tip: Increasing the steps increases the quality of the image with diminishing returns beyond 35, as well as the run time. Only change this if you want to experiment with 'undercooking' the image, or are using a specialized model.
  type: float
  default: 30
  minimum: 1
  maximum: 50
  comfyui: 
    node_id: 1
    field: inputs
    subfield: steps
- name: size_from_input
  label: Size From Input
  description: Override the width and height parameters with the actual resolution of your image. 
  tip: Its best practice to use resolutions included in the models training data. upscaling later can get you to a higher resolution
  type: bool
  default: true
  comfyui: 
    node_id: 1
    field: inputs
    subfield: size_from_input
- name: sdxl_aspect
  label: Aspect
  description: Override the width and height parameters with the best resolution closest to what the model was trained on. This will reduce artifacting and assist in a better image generation. 
  tip: SDXL was trained on images equaling about a megapixel, and generating within these preferred aspect ratios will produce better results
  type: bool
  default: true
  comfyui: 
    node_id: 1
    field: inputs
    subfield: sdxl_aspect
- name: input_img_is_controlnet
  label: Use init image for contolnet
  description: use the input image as the controlnet guidance image
  tip: this will usually be true unless you want to mix guidance from a different image while using an init with denoise < 1.0
  type: bool  
  default: true
  comfyui: 
    node_id: 1
    field: inputs
    subfield: input_img_is_controlnet
- name: preprocessor
  label: Preprocessor
  description: the preprocessor you choose will prepare your input image to guide your diffusion towards that shape
  tip:  depth_midas will recreate the approximate depth of your input scene, and is best for 3 dimensional compositions. canny creates strong edge adherance to the shape of your image, whereas scribble will create guidance towards a rougher shape of your input image. 
  type: string
  default: canny
  choices: [canny, depth_midas, scribble, dwpose]
  comfyui: 
    node_id: 100
    field: inputs
    subfield: preprocessor
- name: controlnet_strength
  label: Controlnet Strength
  description: set the guidance strength of the controlnet model in slot 1
  tip: a good default is around 0.6, with ranges between 0.2 for subtle guidance, and 1.0 for something more heavy handed
  type: float
  default: 0.6
  minimum: 0.0
  maximum: 1.5
  comfyui: 
    node_id: 100
    field: inputs
    subfield: strength
