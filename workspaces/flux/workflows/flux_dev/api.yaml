name: Create an image (Advanced)
description: Generate an image from text using Flux.
thumbnail: app/flux.png
tip: Flux is a text to image model by Black Forest Labs (ex StabilityAI employees ) that excels at prompt coherence, aesthetics and text generation. This tool is a strong choice for generating an image from text! Flux-dev can use ControlNet guidance to maintain the contours or shape outlines of the input image while generating a new style or aesthetic on top of it, and LoRA models to apply a pretrained fine tuned concept. While controlnet exists for this tool, the controlnet models released so far are considered inferior to those from SDXL, and the other tools should be preferred for shape guidance creations. Flux-dev was trained on a dataset ranging in size from .1 to 2 megapixels, and is capable of generating images at higher resolutions than Stable Diffusion without noticable artifacting. While best practice seems to dictate that Flux likes similar resolutions to Stable Diffusion (such as those in the resolutions: list), it is capable of good results higher resolutions, rounded to 64 pixels such as 1920x1088, 1408x1408, 1728x1152, 1664x1216 etc.
output_type: image
status: prod
base_model: flux-dev
cost_estimate: 2 * n_samples
handler: comfyui
resolutions: [21-9_1536x640, 16-9_1344x768, 3-2_1216x832, 4-3_1152x896, 1-1_1024x1024, 3-4_896x1152, 2-3_832x1216, 9-16_768x1344, 9-21_640x1536]
comfyui_output_node_id: 206 
comfyui_intermediate_outputs:
  - name: mapping_motion
    node_id: 342
parameters:
- name: prompt
  label: Prompt
  description: Describe an image
  tip: Flux loves very detailed and descriptive prompts so try to be elaborate. Flux is also great at drawing text in images when requested. Flux uses a T5 text encoder in addition to CLIP, which allows for more conversational human-like natural language prompting. Create great T5 text encoder prompts for Flux-dev by specifying the subject, style, composition, lighting, color palette, mood/atmosphere, technical details, and additional elements to guide the generation of detailed and cohesive visual descriptions.
  type: string
  required: true
  default: 
  comfyui: 
    node_id: 131
    field: inputs
    subfield: value
- name: width
  label: Width
  description: Width in pixels
  required: true
  type: int
  default: 1024
  minimum: 256
  maximum: 2048
  visible_if: use_init_image=false
  step: 8
  comfyui: 
    node_id: 119
    field: inputs
    subfield: width
- name: height
  label: Height
  description: Height in pixels
  required: true
  type: int
  default: 1024
  minimum: 256
  maximum: 2048
  visible_if: use_init_image=false
  step: 8
  comfyui: 
    node_id: 119
    field: inputs
    subfield: height
- name: use_init_image
  label: Add a starting image
  description: Enable image-to-image, and controlnet guidance features
  tip: Using a starting image helps guide your prompted creation towards the colors present in your initial input. If an image is supplied with the prompt or controlnet is employed, this should be true.
  type: bool
  default: false
  comfyui: 
    node_id: 145
    field: inputs
    subfield: value
- name: init_image
  label: Starting image
  description: Initial image from which to start
  tip: This can be used to guide the image generation with colors and shapes of an input image. 
  type: image
  visible_if: use_init_image=true
  comfyui: 
    node_id: 302
    field: inputs
    subfield: image
- name: denoise
  label: AI strength
  description: Strength of the AI generation process on top of the starting image
  tip: Decreasing generation strength increases the influence of a starting image in image-to-image workflows. The default of 1 is 100% AI creativity, ignoring all traces of the starting image, whereas a medium blend of about 50% will maintain close adherance to the original input. You typically want to use this with high denoise values >0.8 for Flux unless you only want only very subtle variations of the starting image.
  type: float
  default: 1.0
  minimum: 0.0
  maximum: 1.0
  visible_if: use_init_image=true
  comfyui: 
    node_id: 323
    field: inputs
    subfield: value
- name: size_from_input
  label: Use starting image size
  description: Override the width and height parameters with the resolution of your starting image. 
  tip: It's best practice to use resolutions included in the models training data. Upscaling later can get you to a higher resolution, but if you need to match the exact image dimensions of your input, this should be true. Keep in mind that all diffusion models will enforce an 8 pixel alignment, so if your input image has a dimension that is not divisible by 8, it will be rounded to the nearest 8 pixel value.
  type: bool
  default: false
  visible_if: use_init_image=true
  comfyui: 
    node_id: 120
    field: inputs
    subfield: value
- name: use_controlnet
  label: Use starting image as shape guide
  description: Activate Controlnet to use the starting image shape in the generation.
  tip: Controlnet guides the generated output towards the shape of the Starting Image.
  type: bool
  default: false
  comfyui: 
    node_id: 133
    field: inputs
    subfield: value
- name: preprocessor
  label: Controlnet preprocessor
  description: Determines the type of shape guidance controlnet model used
  tip: Help the user determine the best preprocessor for their input image. CannyEdgePreprocessor detects thin binary edges and is useful for tasks requiring fast edge detection, such as architectural designs or simple object outlines, but it may struggle with complex scenes or noise. MiDaS-DepthMapPreprocessor generates luminance-based depth maps and is ideal for creating realistic 3D-like compositions, such as emphasizing depth in landscapes or isolating foreground objects. HEDPreprocessor excels at producing smooth, intricate edge detection and is suitable for detailed illustrations, artistic line work, or noise-sensitive scenes. DWPreprocessor, as an OpenPose-based preprocessor, identifies human keypoints and is perfect for generating images with accurate human poses, such as action shots, dance scenes, or character positioning. AnyLineArtPreprocessor_aux converts images into detailed line drawings and is best for black-and-white artwork, manga-style illustrations, or clear contrast outlines. ScribblePreprocessor processes rough sketches into structured inputs, ideal for refining rough concepts or integrating hand-drawn elements into finished compositions with a sketch-like aesthetic.
  type: string
  visible_if: use_controlnet=true
  default: CannyEdgePreprocessor
  choices: [CannyEdgePreprocessor, MiDaS-DepthMapPreprocessor, AnyLineArtPreprocessor_aux, HEDPreprocessor, DWPreprocessor, ScribblePreprocessor]
  choices_labels: [edges (canny), depth, lines (lineart), soft lines (HED), human pose, scribble]
  comfyui: 
    node_id: 104
    field: inputs
    subfield: preprocessor
- name: controlnet_strength
  label: Controlnet strength
  description: set the guidance strength of the controlnet model in slot 1
  tip: A good default for flux is around 0.6, with ranges between 0.2 for subtle guidance, and 1.0 for more heavy handed results.
  type: float
  default: 0.6
  minimum: 0.0
  maximum: 1.5
  visible_if: use_controlnet=true
  comfyui: 
    node_id: 135
    field: inputs
    subfield: value
- name: use_lora
  label: Use a trained model
  description: Apply LoRA finetune model style to image generation
  tip: Models created with Eden LoRA trainer can add people, styles and conceptual embeddings into the diffusion model, giving it an idea of new information provided by the user.
  type: bool
  default: false
  comfyui: 
    node_id: 144
    field: inputs
    subfield: value
- name: lora
  label: Trained model (LoRA)
  description: Use a LoRA finetune on top of the base model.
  type: lora
  visible_if: use_lora=true
  comfyui: 
    node_id: 80
    field: inputs
    subfield: lora_name
- name: lora_strength
  label: Model Strength
  description: Strength of the custom model (LoRA)
  tip: If outputs resemble the LoRA but have low prompt adherence or all look the same, turn down the LoRA strength. A stronger LoRA strength around .9 is better to recreate faces when using a model trained on specific person.
  type: float
  default: 0.7
  minimum: 0.0
  maximum: 1.25
  visible_if: use_lora=true
  comfyui: 
    node_id: 141
    field: inputs
    subfield: value
# - name: use_ipadapter
#   label: Use style image
#   description: Transfer style from image
#   tip: Ipadapter takes compositional concepts, subjects and aesthetic elements from a style image and applies it to the generated image.
#   type: bool
#   default: false
#   comfyui: 
#     node_id: 134
#     field: inputs
#     subfield: value
# - name: style_image
#   label: Style image
#   description: Image of an aesthetic style, texture or visual content transferred to the generated image creation
#   type: image
#   visible_if: use_ipadapter=true
#   comfyui: 
#     node_id: 142
#     field: inputs
#     subfield: image
# - name: ipadapter_strength
#   label: Style strength
#   description: set the strength of the image style transfer
#   tip: Higher values (>0.8) will cause strong adherence to the style image, lower values (<0.3) will give more freedom to interpret the prompt.
#   type: float
#   default: 0.6
#   minimum: 0.0
#   maximum: 1.5
#   visible_if: use_ipadapter=true
#   comfyui: 
#     node_id: 137
#     field: inputs
#     subfield: strength_model
- name: seed
  label: Seed
  description: Set random seed for reproducibility. If blank, will be set to a random value.
  tip: You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this blank! 
  type: int
  default: random
  minimum: 0
  maximum: 2147483647
  comfyui: 
    node_id: 330
    field: inputs
    subfield: seed
- name: n_samples
  label: Number of samples
  description: Number of samples to generate
  tip: This is the number of outputs generated for this prompt. If you get a request for n_samples > 1, you are still using a *single* prompt for the whole set.
  type: int
  default: 1
  minimum: 1
  maximum: 4
