name: Create an image (SDXL)
description: Generate an image from text using the classic SDXL model and trained concepts (LoRAs).
tip: |-
  In most vanilla txt2img cases FLUX should be used instead of this tool.
  This tool uses StableDiffusionXL (SDXL) for image generation, which, while older, can produce creative and artistic results with weird artefacts. It works best with approximately 1-megapixel images, and resolutions should be divisible by 8. The model supports:
  - ControlNet guidance to preserve shapes from input images
  - LoRA models for applying fine-tuned concepts
  - ipAdapter for style transfer from reference images
  While it may struggle with complex prompts and text rendering, it can produce unique and imaginative outputs, especially when using trained SDXL LoRAs. Users can provide an initial image to maintain high-level shape and color characteristics in the final generation.
thumbnail: app/txt2img.png
cost_estimate: 1 * n_samples
output_type: image
resolutions: [21-9_1536x640, 16-9_1344x768, 3-2_1216x832, 4-3_1152x896, 5-4_1088x896, 1-1_1024x1024, 4-5_896x1088, 3-4_896x1152, 2-3_832x1216, 9-16_768x1344, 9-21_640x1536]
handler: comfyui
status: prod
base_model: sdxl
comfyui_output_node_id: 412
comfyui_intermediate_outputs:
  controlnet_signal: 416
parameters:
  prompt:
    type: string
    label: Prompt
    description: Describe an image
    tip: |-
      Effective prompts are concise and specific, starting with a primary subject and its action. They can include context elements like background, secondary items, color schemes, style, mood, lighting, perspective, textures, time period, and cultural elements. End with artistic medium and quality enhancers like "HD" or "masterpiece". Use quotes for text elements.
      IMPORTANT, If the user gives you a short, general, or visually vague prompt, you should augment their prompt by imagining richer details, following the tips above. 
      If a user gives a long, detailed, or well-thought out composition, or requests to have their prompt strictly adhered to without revisions or embellishment, you should adhere to or repeat their exact prompt. 
      The goal is to be authentic to the user's request, but to help them get better results when they are new, unsure, or lazy."
    required: true
    comfyui:
      node_id: 370
      field: inputs
      subfield: body
  width:
    type: integer
    label: Width
    description: Width in pixels
    tip: |-
      Default to using a high resolution of at least 1 megapixel for the image. Use landscape aspect ratio for prompts that are wide or more landscape-oriented, and portrait aspect ratio for tall things. When using portrait aspect ratio, do not exceed 1:1.5 aspect ratio. Only do square if the user requests it. Use your best judgment.
    required: true
    default: 1024
    minimum: 256
    maximum: 2048
    visible_if: use_init_image=false
    step: 8
    comfyui:
      node_id: 7
      field: inputs
      subfield: width
  height:
    type: integer
    label: Height
    description: Height in pixels
    tip: Use the description for width to understand how to use height well.
    required: true
    default: 1024
    minimum: 256
    maximum: 2048
    visible_if: use_init_image=false
    step: 8
    comfyui:
      node_id: 7
      field: inputs
      subfield: height
  n_samples:
    type: integer
    label: Number of samples
    description: Number of samples to generate
    tip: |-
      This is the number of variations to generate for the prompt. If you get a request for n_samples > 1, you are still using a *single* prompt for the whole set.
    required: true
    default: 1
    minimum: 1
    maximum: 4
  use_lora:
    type: boolean
    label: Use custom model
    description: Activate a LoRA finetuned model
    tip: Activate this to generate using a custom, trained LoRA (SDXL) model.
    default: false
    comfyui:
      node_id: 193
      field: inputs
      subfield: value
  lora:
    type: lora
    label: LoRA
    description: Use a custom model (LoRA)
    tip: name of the custom (SDXL) LoRA model.
    visible_if: use_lora=true
    comfyui:
      node_id: 162
      field: inputs
      subfield: lora_name
  lora_strength:
    type: float
    label: Custom model strength
    description: Strength of the custom model (LoRA)
    tip: |-
      If outputs resemble the LoRA but have low prompt adherence or all look the same, turn down the LoRA strength. If outputs dont look enough like the training images, increase the LoRA strength..
    default: 0.7
    minimum: 0.0
    maximum: 1.0
    step: 0.01
    visible_if: use_lora=true
    comfyui:
      node_id: 96
      field: inputs
      subfield: value
  use_init_image:
    type: boolean
    label: Use starting image
    description: Enable image-to-image, and controlnet guidance features
    tip: |-
      Using a starting image helps guide your prompted creation towards the elements present in your initial input. if an image is supplied with the prompt or controlnet is employed, this should be always betrue.
    default: false
    comfyui:
      node_id: 198
      field: inputs
      subfield: value
  init_image:
    type: image
    label: Starting image
    description: Initial image from which to start diffusion process
    tip: |-
      a starting image will set the base for the diffusion process to generate from, and will guide the output towards the colors and general composition of the starting image. This is useful for image-to-image tasks.
    visible_if: use_init_image=true
    comfyui:
      node_id: 16
      field: inputs
      subfield: image
  denoise:
    type: float
    label: Generation strength
    description: Strength of the generation process on top of the starting image
    tip: |-
      Decreasing generation strength increases the influence of a starting image in image-to-image workflows. The default of 1 is 100% AI creativity, ignoring all traces of the starting image, whereas a medium blend of about 50% will maintain close adherance to the original input. Denoise should always stay at 1 unless an init image is employed; never use less than 1 unless you're specifically trying to do an img2img task. In scenario in which an image2image task is requested, the init_image should be true, and a starting image should be supplied. Never set this parameter to true unless an init image is provided.
    default: 1.0
    minimum: 0.0
    maximum: 1.0
    step: 0.01
    visible_if: use_init_image=true
    comfyui:
      node_id: 220
      field: inputs
      subfield: value
  size_from_input:
    type: boolean
    label: Use starting image size
    description: |-
      Override the provided width and height parameters with the actual resolution of your starting image.
    tip: |-
      It's best practice to use resolutions included in the models training data. Upscaling later can get you to a higher resolution and is almost always better than generating directly at a high resolution. Keep in mind that stable diffusion will always scale dimensions to the nearest divisible by 8 resolution, so if an oddly shaped image is supplied, the output will preserve aspect ratio as closely as possible, but the dimensions will not match the input exactly.
    default: false
    visible_if: use_init_image=true
    comfyui:
      node_id: 8
      field: inputs
      subfield: value
  enforce_SDXL_resolution:
    type: boolean
    label: Use optimized resolution
    description: Round to the nearest optimized SDXL resolution, recommended.
    tip: |-
      It's best practice to use resolutions included in the models training data. This option will fill or crop the input image to a legal SDXL resolution, producing results that are less likely to contain the artifacts typical of an unsupported resolution.
    default: false
    visible_if: use_init_image=true
    comfyui:
      node_id: 337
      field: inputs
      subfield: value
  use_controlnet:
    type: boolean
    label: Apply controlnet to starting image
    visible_if: use_init_image=true
    description: |-
      Apply Controlnet guidance to the image generation to guide the generated image towards the shape of the starting image.
    tip: |-
      Controlnet uses an assortment of image preprocessors to guide the output results towards the shape of a Starting Image. This parameter should never be true if no starting image is supplied, and should always be true if an init image is supplied and the user's request suggests they want guidance towards the shape of the input image. Keep in mind that a non-technical user may not have the vocabulary to describe what they want, and interpret their request with your best judgement as to whether or not they want shape guidance. Shape guidance is a user friendly term used in Eden tools to refer to the application of controlnet conditioning.
    default: false
    comfyui:
      node_id: 408
      field: inputs
      subfield: value
  preprocessor:
    type: string
    label: Controlnet preprocessor
    description: |-
      What type of shape guidance to apply. Different types have subtly different effects on the output.
    tip: |-
      Use best judgement based on the supplied image and user request to select the correct controlnet preprocessor and model to suit their needs. Canny detects thin binary edges and is useful for tasks requiring fast edge detection, such as architectural designs or simple object outlines, but it may struggle with complex scenes or noise. Depth generates luminance-based depth maps and is ideal for creating realistic 3D-like compositions, such as emphasizing depth in landscapes or isolating foreground objects. HED excels at producing smooth, intricate edge detection and is suitable for detailed illustrations, artistic line work, or noise-sensitive scenes. OpenPose identifies human keypoints, making it a perfect choice for generating images with accurate human poses, such as action shots, dance scenes, or character positioning. LineArt converts images into detailed line drawings and is best for black-and-white artwork, manga-style illustrations, or clear contrast outlines. Scribble processes rough sketches into structured inputs, ideal for refining rough concepts or integrating hand-drawn elements into finished compositions with a sketch-like aesthetic.
    default: canny
    choices: [none, canny, depth, scribble, pose, threshold, lineart]
    visible_if: use_controlnet=true
    comfyui:
      node_id: 283
      field: inputs
      subfield: select_item
  controlnet_strength:
    type: float
    label: Controlnet strength
    description: Strength of the shape guidance (controlnet)
    tip: |-
      A good default is around 0.6, with ranges around 0.4 for more subtle guidance (more freedom for the AI model), and 0.8-1.0 for something more heavy handed.
    default: 0.6
    minimum: 0.0
    maximum: 1.2
    step: 0.01
    visible_if: use_controlnet=true
    comfyui:
      node_id: 36
      field: inputs
      subfield: value
  use_ipadapter:
    type: boolean
    label: Use style image
    description: Apply style elements from a given image
    tip: |-
      Ipadapter takes compositional concepts, subjects and aesthetic elements from a style image and applies it to the generated image. It essentially interprets prompt conditioning from the use of an input image, and is often referred to vernacularly throughout Eden tools as a style image. A non-technical user may not have the vocabulary to describe what they want, interpret their request with your best judgement as to whether or not they want style guidance. Style guidance is a user friendly term used in Eden tools to refer to the application of ipAdapter conditioning.
    default: false
    comfyui:
      node_id: 35
      field: inputs
      subfield: value
  style_image:
    type: image
    label: Style image
    description: |-
      Image of an aesthetic style, texture or visual content to be transferred to the generated image
    visible_if: use_ipadapter=true
    comfyui:
      node_id: 145
      field: inputs
      subfield: image
  ipadapter_strength:
    type: float
    label: Style strength
    description: set the strength of the image style transfer
    tip: |-
      Higher values (>0.8) will cause strong adherence to the style image, lower values (<0.4) will give more freedom to interpret the prompt.
    default: 0.8
    minimum: 0.0
    maximum: 1.5
    visible_if: use_ipadapter=true
    comfyui:
      node_id: 59
      field: inputs
      subfield: value
  SDXL_model:
    type: string
    label: SDXL Model
    description: Pick which SDXL model to use
    hide_from_agent: true
    default: Eden_SDXL.safetensors
    choices: [Eden_SDXL.safetensors, juggernautXL_version6Rundiffusion.safetensors]
    comfyui:
      node_id: 410
      field: inputs
      subfield: ckpt_name
  no_token_prompt:
    type: string
    label: Prompt without the added embedding trigger.
    description: Base user prompt without the embedding token
    hide_from_agent: true
    hide_from_ui: true
    comfyui:
      node_id: 243
      field: inputs
      subfield: value
  negative_prompt:
    type: string
    label: Negative prompt
    description: Negative text prompt, what you do *not* want to generate.
    tip: |-
      Keep this at default or at most, add to this prompt, unless specifically instructed otherwise.
    hide_from_agent: true
    default: embedding:negativeXL_D, (worst quality, low quality, normal quality:1.5)
    comfyui:
      node_id: 33
      field: inputs
      subfield: value
  seed:
    type: integer
    label: Seed
    description: Set random seed for reproducibility. If blank, will be set to a random value.
    tip: |-
      You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this blank. Great for redoing an image generation you like with a slightly modified prompt, but otherwise you should usually leave this blank.
    default: random
    minimum: 0
    maximum: 2147483647
    comfyui:
      node_id: 27
      field: inputs
      subfield: seed
  steps:
    type: integer
    label: Steps
    description: Set the number of diffusion steps
    tip: |-
      The default value is sufficient for most jobs, but at a speed cost can be increased, or decreased to undercook the image generation.
    default: 35
    step: 1
    minimum: 10
    maximum: 50
    comfyui:
      node_id: 171
      field: inputs
      subfield: steps_total
