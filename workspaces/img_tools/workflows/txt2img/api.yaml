name: Create an image (SDXL)
description: Generate an image from text using the classic SDXL model and trained concepts.
thumbnail: app/txt2img.png
tip: This is the legacy (battlehardened) function to generate an image from text a prompt. The outputs are not as detailed as FLUX, but often more creative and weird. With ControlNet guidance one can maintain the contours or shape outlines of an input image while generating a new style or aesthetic on top of it, LoRA models to apply a pretrained fine tuned concept, and ipAdapter functionality to transfer the style from a Style Image to the output result. This tool uses SDXL, which as of now is the best model to use for guided image generation, and this tool should be chosen for image generations that match the unique characteristics of this model, which is decent at styles, NSFW, and has a good knowledge of celebrities and artists that newer base models lack context for. Users can supply a starting init input image and use controlnet guidance and or a latent encoding of the supplied input image, in which case shape and color characteristics from the supplied image will stay present in the resultant generation. Stable Diffusion XL is trained on a dataset of images approximately 1 megapixel in size, and the provided resolutions list is the set of legal resolutions for this model, and should always be used unless a user requests a specific resolution. If an aspect ratio is specifically requested, always aim for dimensions that total approximately 1 megapixel and round to numbers that are divisible by 8.
output_type: image
status: prod
private: false
visible: true
cost_estimate: 1 * n_samples
handler: comfyui
base_model: SDXL
resolutions: [21-9_1536x640, 16-9_1344x768, 3-2_1216x832, 4-3_1152x896, 5-4_1088x896, 1-1_1024x1024, 4-5_896x1088, 3-4_896x1152, 2-3_832x1216, 9-16_768x1344, 9-21_640x1536]
comfyui_output_node_id: 412
comfyui_intermediate_outputs:
  - name: controlnet_signal
    node_id: 416
parameters:
- name: prompt
  label: Prompt
  description: Describe an image
  tip: Prompts are specific, detailed, concise, and visually descriptive, avoiding unnecessary verbosity and abstract, generic terms. They usually have ● Primary subject (i.e., person, character, animal, object, ...), e.g "Renaissance noblewoman", "alien starship". Should appear early in prompt. ● Action of the subject, e.g. "holding an ancient book", "orbiting a distant planet", if there is no main subject, a good context description will do here (what is going on in the scene?). They sometimes have several stylistic modifiers near the end of the prompt, such as ● Background, environment or context surrounding the subject, e.g. "in a dimly lit Gothic castle", "in a futuristic 22nd century space station". ● Secondary items that enhance the subject or story. e.g. "wearing an intricate lace collar", "standing next to a large, ancient tree". ● Color schemes, e.g. "shades of deep red and gold", "monochrome palette with stark contrasts", "monochrome", ... ● Style or method of rendering, e.g. "reminiscent of Vermeer's lighting techniques", "film noir", "cubism", ... ● Mood or atmospheric quality e.g. "atmosphere of mystery", "serene mood". ● Lighting conditions e.g. "bathed in soft, natural window light", "dramatic shadows under a spotlight". ● Perspective or or viewpoint, e.g. "bird's eye view", "from a low angle", "fisheye", .. ● Textures or materials, e.g. "textures of rich velvet and rough stone". ● Time Period, e.g. "Victorian Era", "futuristic 22nd century". ● Cultural elements, e.g. "inspired by Norse mythology", "traditional Japanese setting". ● Artistic medium, e.g. "watercolor painting", "crisp digital Unreal Engine rendering", "8K UHD professional photo", "cartoon drawing", ... They often end with trigger words that improve images in a general way, e.g. "High Resolution", "HD", "sharp details", "masterpiece", "stunning composition", ... If the prompt contains a request to render text, enclose the text in quotes, e.g. A Sign with the text “Peace”. IMPORTANT, If the user gives you a short, general, or visually vague prompt, you should augment their prompt by imagining richer details, following the tips above. If a user gives a long, detailed, or well-thought out composition, or requests to have their prompt strictly adhered to without revisions or embellishment, you should adhere to or repeat their exact prompt. The goal is to be authentic to the user's request, but to help them get better results when they are new, unsure, or lazy.
  type: string
  required: true
  comfyui: 
    node_id: 370
    field: inputs
    subfield: body
- name: width
  label: Width
  description: Width in pixels
  tip: Default to using a high resolution of at least 1 megapixel for the image. Use landscape aspect ratio for prompts that are wide or more landscape-oriented, and portrait aspect ratio for tall things. When using portrait aspect ratio, do not exceed 1:1.5 aspect ratio. Only do square if the user requests it. Use your best judgment.
  type: int
  required: true
  default: 1024
  minimum: 256
  maximum: 2048
  visible_if: use_init_image=false
  step: 8
  comfyui: 
    node_id: 7
    field: inputs
    subfield: width
- name: height
  label: Height
  description: Height in pixels
  tip: Use the description for width to understand how to use height well.
  type: int
  required: true
  default: 1024
  minimum: 256
  maximum: 2048
  visible_if: use_init_image=false
  step: 8
  comfyui: 
    node_id: 7
    field: inputs
    subfield: height
- name: n_samples
  label: Number of samples
  description: Number of samples to generate
  tip: This is the number of variations to generate for the prompt. If you get a request for n_samples > 1, you are still using a *single* prompt for the whole set.
  type: int
  required: true
  default: 1
  minimum: 1
  maximum: 4
- name: use_lora
  label: Use custom model
  description: Activate a LoRA finetuned model
  tip: Activate this to generate using a custom, trained LoRA (SDXL) model.
  type: bool
  default: false
  comfyui: 
    node_id: 193
    field: inputs
    subfield: value
- name: lora
  label: LoRA
  description: Use a custom model (LoRA)
  tip: name of the custom (SDXL) LoRA model.
  type: lora
  visible_if: use_lora=true
  comfyui: 
    node_id: 162
    field: inputs
    subfield: lora_name
- name: lora_strength
  label: Custom model strength
  description: Strength of the custom model (LoRA)
  tip: If outputs resemble the LoRA but have low prompt adherence or all look the same, turn down the LoRA strength. If outputs dont look enough like the training images, increase the LoRA strength..
  type: float
  default: 0.7
  minimum: 0.0
  maximum: 1.0
  visible_if: use_lora=true
  comfyui: 
    node_id: 96
    field: inputs
    subfield: value
- name: use_init_image
  label: Use starting image
  description: Enable image-to-image, and controlnet guidance features
  tip: Using a starting image helps guide your prompted creation towards the elements present in your initial input. if an image is supplied with the prompt or controlnet is employed, this should be always betrue.
  type: bool
  default: false
  comfyui: 
    node_id: 198
    field: inputs
    subfield: value
- name: init_image
  label: Starting image
  description: Initial image from which to start diffusion process
  tip: a starting image will set the base for the diffusion process to generate from, and will guide the output towards the colors and general composition of the starting image. This is useful for image-to-image tasks.
  type: image
  visible_if: use_init_image=true
  comfyui: 
    node_id: 16
    field: inputs
    subfield: image
- name: denoise
  label: Generation strength
  description: Strength of the generation process on top of the starting image
  tip: Decreasing generation strength increases the influence of a starting image in image-to-image workflows. The default of 1 is 100% AI creativity, ignoring all traces of the starting image, whereas a medium blend of about 50% will maintain close adherance to the original input. Denoise should always stay at 1 unless an init image is employed; never use less than 1 unless you're specifically trying to do an img2img task. In scenario in which an image2image task is requested, the init_image should be true, and a starting image should be supplied. Never set this parameter to true unless an init image is provided.
  type: float
  default: 1.0
  minimum: 0.0
  maximum: 1.0
  visible_if: use_init_image=true
  comfyui: 
    node_id: 220
    field: inputs
    subfield: value
- name: size_from_input
  label: Use starting image size
  description: Override the provided width and height parameters with the actual resolution of your starting image. 
  tip: It's best practice to use resolutions included in the models training data. Upscaling later can get you to a higher resolution and is almost always better than generating directly at a high resolution. Keep in mind that stable diffusion will always scale dimensions to the nearest divisible by 8 resolution, so if an oddly shaped image is supplied, the output will preserve aspect ratio as closely as possible, but the dimensions will not match the input exactly.
  type: bool
  default: false
  visible_if: use_init_image=true
  comfyui: 
    node_id: 8
    field: inputs
    subfield: value
- name: enforce_SDXL_resolution
  label: Use optimized resolution
  description: Round to the nearest optimized SDXL resolution, recommended.
  tip: It's best practice to use resolutions included in the models training data. This option will fill or crop the input image to a legal SDXL resolution, producing results that are less likely to contain the artifacts typical of an unsupported resolution.
  type: bool
  default: false
  visible_if: use_init_image=true
  comfyui: 
    node_id: 337
    field: inputs
    subfield: value
- name: use_controlnet
  label: Apply controlnet to starting image
  visible_if: use_init_image=true
  description: Apply Controlnet guidance to the image generation to guide the generated image towards the shape of the starting image.
  tip: Controlnet uses an assortment of image preprocessors to guide the output results towards the shape of a Starting Image. This parameter should never be true if no starting image is supplied, and should always be true if an init image is supplied and the user's request suggests they want guidance towards the shape of the input image. Keep in mind that a non-technical user may not have the vocabulary to describe what they want, and interpret their request with your best judgement as to whether or not they want shape guidance. Shape guidance is a user friendly term used in Eden tools to refer to the application of controlnet conditioning.
  type: bool
  default: false
  comfyui: 
    node_id: 408
    field: inputs
    subfield: value
- name: preprocessor
  label: Controlnet preprocessor
  description: What type of shape guidance to apply. Different types have subtly different effects on the output.
  tip: Use best judgement based on the supplied image and user request to select the correct controlnet preprocessor and model to suit their needs. Canny detects thin binary edges and is useful for tasks requiring fast edge detection, such as architectural designs or simple object outlines, but it may struggle with complex scenes or noise. Depth generates luminance-based depth maps and is ideal for creating realistic 3D-like compositions, such as emphasizing depth in landscapes or isolating foreground objects. HED excels at producing smooth, intricate edge detection and is suitable for detailed illustrations, artistic line work, or noise-sensitive scenes. OpenPose identifies human keypoints, making it a perfect choice for generating images with accurate human poses, such as action shots, dance scenes, or character positioning. LineArt converts images into detailed line drawings and is best for black-and-white artwork, manga-style illustrations, or clear contrast outlines. Scribble processes rough sketches into structured inputs, ideal for refining rough concepts or integrating hand-drawn elements into finished compositions with a sketch-like aesthetic.
  type: string
  default: canny
  choices: [none, canny, depth, scribble, pose, threshold, lineart]
  visible_if: use_controlnet=true
  comfyui: 
    node_id: 283
    field: inputs
    subfield: select_item
- name: controlnet_strength
  label: Controlnet strength
  description: Strength of the shape guidance (controlnet)
  tip: A good default is around 0.6, with ranges around 0.4 for more subtle guidance (more freedom for the AI model), and 0.8-1.0 for something more heavy handed.
  type: float
  default: 0.6
  minimum: 0.0
  maximum: 1.2
  visible_if: use_controlnet=true
  comfyui: 
    node_id: 36
    field: inputs
    subfield: value
- name: use_ipadapter
  label: Use style image
  description: Apply style elements from a given image
  tip: Ipadapter takes compositional concepts, subjects and aesthetic elements from a style image and applies it to the generated image. It essentially interprets prompt conditioning from the use of an input image, and is often referred to vernacularly throughout Eden tools as a style image. A non-technical user may not have the vocabulary to describe what they want, interpret their request with your best judgement as to whether or not they want style guidance. Style guidance is a user friendly term used in Eden tools to refer to the application of ipAdapter conditioning.
  type: bool
  default: false
  comfyui: 
    node_id: 35
    field: inputs
    subfield: value
- name: style_image
  label: Style image
  description: Image of an aesthetic style, texture or visual content to be transferred to the generated image
  type: image
  visible_if: use_ipadapter=true
  comfyui: 
    node_id: 145
    field: inputs
    subfield: image
- name: ipadapter_strength
  label: Style strength
  description: set the strength of the image style transfer
  tip: Higher values (>0.8) will cause strong adherence to the style image, lower values (<0.4) will give more freedom to interpret the prompt.
  type: float
  default: 0.8
  minimum: 0.0
  maximum: 1.5
  visible_if: use_ipadapter=true
  comfyui: 
    node_id: 59
    field: inputs
    subfield: value
- name: SDXL_model
  label: SDXL Model
  description: Pick which SDXL model to use
  hide_from_agent: true
  type: string
  default: Eden_SDXL.safetensors
  choices: [Eden_SDXL.safetensors, juggernautXL_version6Rundiffusion.safetensors]
  comfyui: 
    node_id: 410
    field: inputs
    subfield: ckpt_name
- name: no_token_prompt
  label: Prompt without the added embedding trigger.
  description: Base user prompt without the embedding token
  type: string
  hide_from_agent: true
  hide_from_ui: true
  comfyui: 
    node_id: 243
    field: inputs
    subfield: value
- name: negative_prompt
  label: Negative prompt
  description: Negative text prompt, what you do *not* want to generate.
  tip: Keep this at default or at most, add to this prompt, unless specifically instructed otherwise.
  hide_from_agent: true
  type: string
  default: embedding:negativeXL_D, (worst quality, low quality, normal quality:1.5)
  comfyui: 
    node_id: 33
    field: inputs
    subfield: value
- name: seed
  label: Seed
  description: Set random seed for reproducibility. If blank, will be set to a random value.
  tip: You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this blank. Great for redoing an image generation you like with a slightly modified prompt, but otherwise you should usually leave this blank.
  type: int
  default: random
  minimum: 0
  maximum: 2147483647
  comfyui: 
    node_id: 27
    field: inputs
    subfield: seed
- name: steps
  label: Steps
  description: Set the number of diffusion steps
  tip: The default value is sufficient for most jobs, but at a speed cost can be increased, or decreased to undercook the image generation. 
  type: int
  default: 35
  step: 1
  minimum: 10
  maximum: 50
  comfyui: 
    node_id: 171
    field: inputs
    subfield: steps_total