name: TextureFlow
description: Mix multiple images into a video, optionally add a driving motion or shape.
thumbnail: app/style_mixing_opt.mp4
tip: |-
  This workflow creates smooth, trippy, artistic animations from a set of style (texture) images (and other optional inputs).
  One to six images form the basis of this tool, driving the texture and content of the final animation. In general, the video length should be scaled proportionally with the number of style images, 3s of animation per style image is a good default.
  The motion mapping template controls how the content of the style images is mapped onto the video (spatially). When doing quick experimentation with settings you can set upscale:false to produce a fast but low resolution test render, once a sweet spot is found, you can set upscale:true to generate a full HD final output video.
  There is an optional control_input (set use_controlnet1 to true) and preprocessor1 type that can add specific shape/motion guidance to the generated video like embedding the contours of a logo or word into the video or eg applying the lines or perceived depth of a projection surface when doing videomapping. 
  In general, running TextureFlow without a control_input almost always produces visually appealing results, adding a control_input is tricky. The wrong combination of control_input and style images may look bad (especially when the control strength is too high),
  but the right combo can look absolutely amazing and is one of Edens special endpoints that make our platform unique!
  Some example use cases for TextureFlow:
  - abstract, artistic image animation using a single style image (this will completely deform the input image and lose lots of details but its great to generate smooth, animated textures)
  - mixing multiple style_images using various motion mapping modes to create slick, looping VJ content (abstract, artistic animations)
  - using a logo image as control_input + texture image(s) can create really cool logo animations. 
  - using a simple motion input video or gif (with simple lines / edges / contours) is a great way to drive unique animations (and add new textures with style images)
  - TextureFlow can even produce animated QR codes by setting the QR code image as control_input and using the "None" controlnet (which is a luminance controlnet). Make sure to set the controlnet_strength1 high enough to make the QR code animation scanable.
output_type: video
status: prod
cost_estimate: |-
  0.4 * (1 + n_seconds) * (width + height)/(2*640) * (2 + n_steps) * (upscale ? 1.75 : 1)
base_model: sd1.5
handler: comfyui
resolutions: [16-9_928x512, 3-2_864x576, 1-1_640x640, 2-3_576x864, 9-16_512x928]
comfyui_output_node_id: 446
comfyui_intermediate_outputs:
  - name: mapping_motion
    node_id: 199
  - name: control_signal_1
    node_id: 501
parameters:
- name: images
  label: Style Images
  description: Style images driving the content of the generated video (through IP-adapter)
  type: image[]
  min_length: 1
  max_length: 6
  required: true
  comfyui: 
    node_id: 74
    field: inputs
    subfield: folder
    preprocessing: folder
- name: n_seconds
  label: Video length (seconds)
  description: Number of seconds of video to generate.
  type: float
  default: 6.0
  minimum: 2.0
  maximum: 24.0
  required: true
  step: 0.5
  comfyui: 
    node_id: 570
    field: inputs
    subfield: value
- name: width
  label: Width
  description: Width of the video generation in pixels. High values sometimes cause duplication artefacts like multiple faces etc.
  type: int
  default: 640
  minimum: 384
  maximum: 1024
  required: true
  step: 32
  comfyui: 
    node_id: 261
    field: inputs
    subfield: width
- name: height
  label: Height
  description: Height of the video generation in pixels. High values sometimes cause duplication artefacts like multiple faces etc.
  type: int
  default: 640
  minimum: 384
  maximum: 1024
  required: true
  step: 32
  comfyui: 
    node_id: 261
    field: inputs
    subfield: height
- name: upscale
  label: Activate upscale
  description: Upscale the resulting animation to full HD. (You can turn this off for faster and cheaper experimentation)
  tip: This toggle can optionally be disactivated when you're not sure that the result is going to be good and are still exploring the best settings.
  type: bool
  default: true
  required: true
  comfyui: 
    node_id: 415
    field: inputs
    subfield: value
- name: upscale_resolution
  label: Max(w,h) post upscale
  hide_from_ui: true
  hide_from_agent: true
  description: Number of pixels to upscale to
  type: int
  default: 1280
  comfyui: 
    node_id: 568
    field: inputs
    subfield: value
- name: use_controlnet1
  label: Add shape input
  description: Apply Controlnet shape guidance.
  tip: Controlnet uses image preprocessors to guide the output results towards the shape of a Guidance Image.
  type: bool
  default: false
  comfyui: 
    node_id: 273
    field: inputs
    subfield: value
- name: control_input
  label: Shape input
  description: A shape image/GIF/video used as input to the controlnet. Will guide the shape of the output video. 
  type: image|video
  visible_if: use_controlnet1=true
  comfyui: 
    node_id: 552
    field: inputs
    subfield: path
- name: preprocessor1
  label: Shape guidance type
  description: Type of shape guidance for controlnet. Examples can be seen at https://github.com/lllyasviel/ControlNet-v1-1-nightly?tab=readme-ov-file#controlnet-11-depth
  tip:  In most cases, the default CannyEdgePreprocessor at strengths of 0.4-0.6 is great! Depth will try to maintain the perceived depth of the input scene. Canny edge creates strong edges adhering to the shape of your image, whereas scribble will create guidance towards a rougher sketched shape of your starting image and often produces better quality video at the cost of less finegrained correspondence with the control image. Pose will try to extract the pose from a person and inject it into the video. And None refers to QRmonster which simply maintains luminance of the input (dark and bright regions)
  type: string
  visible_if: use_controlnet1=true
  default: CannyEdgePreprocessor
  choices: [CannyEdgePreprocessor, DepthAnythingV2Preprocessor, AnyLineArtPreprocessor_aux, DensePosePreprocessor, Scribble_XDoG_Preprocessor, none]
  choices_labels: [Edges (Canny), Depth, Lineart, human pose, Scribble lines, Luminance (QR-code, dark/bright patterns)]
  comfyui: 
    node_id: 406
    field: inputs
    subfield: preprocessor    
    remap:
      - node_id: 107
        field: inputs
        subfield: control_net_name
        value:
          - input: CannyEdgePreprocessor
            output: control_v11p_sd15_canny.pth
          - input: DepthAnythingV2Preprocessor
            output: control_v11f1p_sd15_depth.pth
          - input: AnyLineArtPreprocessor_aux
            output: control_v11p_sd15_lineart.pth
          - input: DensePosePreprocessor
            output: control_v11p_sd15_openpose.pth
          - input: Scribble_XDoG_Preprocessor
            output: control_v11p_sd15_scribble.pth
          - input: none
            output: controlnetQRPatternQR_v2Sd15.safetensors
- name: controlnet_strength1
  label: Shape strength
  description: set the guidance strength of the controlnet model, recommended values are 0.4-0.6
  tip: A good default is around 0.5, with ranges between 0.35-0.5 for subtle guidance, and 0.5-0.8 for something more heavy handed
  type: float
  default: 0.5
  minimum: 0.0
  maximum: 1.5
  step: 0.01
  visible_if: use_controlnet1=true
  comfyui: 
    node_id: 116
    field: inputs
    subfield: strength
- name: control_input_fit_strategy
  label: Fit strategy
  description: How should your shape input get resized to the target width/height of the generation?
  tip: fill / crop will cut off edges, stretch will distort the control_input and pad can cause strong visual artefacts (not recommended).
  visible_if: use_controlnet1=true
  type: string
  default: fill / crop
  choices: [stretch, fill / crop, pad]
  comfyui: 
    node_id: 262
    field: inputs
    subfield: method
- name: mapping_mode
  label: Style Mapping Mode
  description: Controls the motion by which the style images will get mapped into the video
  type: string
  choices: [concentric_circles_inwards, concentric_circles_outwards, 
                          concentric_rectangles_inwards, concentric_rectangles_outwards, 
                          rotating_segments_clockwise, rotating_segments_counter_clockwise, 
                          pushing_segments_clockwise, pushing_segments_counter_clockwise, 
                          vertical_stripes_left, vertical_stripes_right, 
                          horizontal_stripes_up, horizontal_stripes_down]
  default: concentric_circles_outwards
  comfyui: 
    node_id: 536
    field: inputs
    subfield: mode
- name: n_steps
  label: Generation steps
  description: Number of LCM denoising steps. Lower is cheaper & faster (for experimenting with settings), higher can sometimes yield slightly better quality.
  type: int
  default: 8
  minimum: 4
  maximum: 14
  step: 1
  comfyui: 
    node_id: 524
    field: inputs
    subfield: value
- name: motion_scale
  label: Motion Strength
  description: Motion scale for the animation, controls how much animated motion will be generated. Lower this value for more gentle, subtle motion. Highly recommended to stay in range [0.9-1.2]
  tip: The default value of 1.1 is good for most cases. Lowering to eg 0.9 will result in very subtle texture motion, increasing to eg 1.25 will result in more motion. Going above 1.3 is almost never desirable.
  type: float
  default: 1.1
  minimum: 0.7
  maximum: 1.4
  step: 0.01
  comfyui: 
    node_id: 450
    field: inputs
    subfield: float_val
- name: feathering_fraction
  label: Boundary softness
  description: Controls how sharp/soft the boundary is between different style regions in the video. 0.0 will cause perfectly outlined style regions, 0.25 will have much smoother transitions.
  type: float
  default: 0.10
  minimum: 0.00
  maximum: 0.25
  step: 0.01
  comfyui: 
    node_id: 136
    field: inputs
    subfield: feathering_fraction
- name: seed
  label: Seed
  description: Set random seed for reproducibility. If blank, will be set to a random value.
  tip: You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this at default! 
  type: int
  default: random
  minimum: 0
  maximum: 2147483647
  comfyui: 
    node_id: 282
    field: inputs
    subfield: seed